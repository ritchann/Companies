{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbeekQqBGjfH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from google.colab import drive\n",
        "from nltk import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.utils.fixes import sklearn\n",
        "from scipy.linalg.decomp import inf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1988\n",
        "\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "sklearn.random.seed(SEED)\n",
        "SEED"
      ],
      "metadata": {
        "id": "zS9OXsmKKkgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "SR2wBo1wKknr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/gdrive/MyDrive/Сompanies/train.csv\")      #чтение train.csv"
      ],
      "metadata": {
        "id": "DbaNx1XbKzwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install locationtagger"
      ],
      "metadata": {
        "id": "HsFEdzpSLB6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "1BQVkg9CL9YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locationtagger\n",
        "\n",
        "def cleanText(value):\n",
        "  value = re.sub(r'\\([^()]*\\)', '', value)      #удаление скобок () и внутреннего текста\n",
        "  value = re.sub(r\"\\d+\", \"\", value, flags=re.UNICODE)      #удаление цифр\n",
        "\n",
        "  value = value.lower()      #приводим к нижнему регистру\n",
        "\n",
        "  for ch in ['&', 'corporation', 'group', '*', ',', 'ооо', '\"', '/', \"'\"]:      #удаление символов и некоторых слов\n",
        "   value = value.replace(ch,'')\n",
        "\n",
        "  array = value.split()\n",
        "  result = []\n",
        "\n",
        "  for word in array:\n",
        "    entities = locationtagger.find_locations(text = word)      #поиск названия городов и стран\n",
        "\n",
        "    if len(array) == 1:\n",
        "      result.append(word)\n",
        "    elif (len(word) > 1) and ('.' not in word) and (len(entities.countries) == 0) and (len(entities.cities) == 0):\n",
        "      result.append(word)\n",
        "\n",
        "  resultString = ' '.join(result)\n",
        "\n",
        "  return resultString"
      ],
      "metadata": {
        "id": "WY_NxlDMK-Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#берем каждое название компании, преобразуем его и составляем csv с оригинальным названием и преобразованным\n",
        "#готовый файл на 39998 записей можно взять https://drive.google.com/drive/folders/18I9-B__uaw9SzEuU9x_1pLC74MFWYYjy?usp=sharing\n",
        "length = len(train)        \n",
        "\n",
        "rows = []\n",
        "header = ['index', 'original', 'transformed']\n",
        "\n",
        "for i in range(len(train[1:length])):\n",
        "    row = train.iloc[i]\n",
        "    name1 = row['name_1']\n",
        "    name2 = row['name_2']\n",
        "\n",
        "    transformed1 = cleanText(name1)\n",
        "    transformed2 = cleanText(name2)   \n",
        "\n",
        "    rows.append([i, name1, transformed1])\n",
        "    rows.append([i, name2, transformed2])\n",
        "\n",
        "with open('/content/transformed_train.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "     writer = csv.writer(f)\n",
        "\n",
        "     writer.writerow(header)\n",
        "\n",
        "     writer.writerows(rows)"
      ],
      "metadata": {
        "id": "0nhntRkoLtmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_train = pd.read_csv(\"/content/gdrive/MyDrive/Сompanies/transformed_train.csv\")\n",
        "transformed_train"
      ],
      "metadata": {
        "id": "rsi0YVAxMCTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybind11\n",
        "!pip install fastwer"
      ],
      "metadata": {
        "id": "8LJDUbotNNdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Levenshtein Distance - CER "
      ],
      "metadata": {
        "id": "5XvmnVBmNZm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fastwer\n",
        "\n",
        "sumTrue = 0\n",
        "length = len(train)\n",
        "\n",
        "for i in range(len(train[0:length])): #с помощью расстояния Левенштейна с порогом CER > 10 сравниваем два обработанных названия, что помогает оценить одинаковые они или нет\n",
        "    row = train.iloc[i]\n",
        "\n",
        "    name1 = cleanText(row.name_1)\n",
        "    name2 = cleanText(row.name_2)\n",
        "\n",
        "    cer = fastwer.score_sent(name1, name2, char_level=True)\n",
        "\n",
        "    if cer == inf:\n",
        "      cer = 0\n",
        "\n",
        "    isDuplicate = True\n",
        "\n",
        "    if(cer > 10):\n",
        "      isDuplicate = False\n",
        "\n",
        "    if(int(isDuplicate) == row.is_duplicate):\n",
        "      sumTrue += 1\n",
        "\n",
        "print(\"Accuracy is: \", sumTrue/len(train[0:length]))"
      ],
      "metadata": {
        "id": "rLpfw9FsNVSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF & KMeans & Levenshtein Distance - CER"
      ],
      "metadata": {
        "id": "LsYA1KphPEi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_train['transformed'] = transformed_train['transformed'].astype(str)"
      ],
      "metadata": {
        "id": "o-aWHyYSP1Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words = 'english') \n",
        "X = vectorizer.fit_transform(transformed_train.transformed.to_list()) "
      ],
      "metadata": {
        "id": "zej-ri-YPYxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_k = 20\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
        "model.fit(X)"
      ],
      "metadata": {
        "id": "WoGsgataP_Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = cleanText(\"JX Nippon Oil & Gas Exploration (Brasil) Ltda\")     #название, для которого мы хотим найти похожее, в результате получаем список похожих названий\n",
        "Y = vectorizer.transform([search])\n",
        "orgPrediction = model.predict(Y)\n",
        "length = len(transformed_train)\n",
        "\n",
        "\n",
        "for i in range(len(transformed_train[0:length])):\n",
        "    row = transformed_train.iloc[i]\n",
        "\n",
        "    text = row.transformed         \n",
        "\n",
        "    vectorText = vectorizer.transform([text])\n",
        "    predictionText = model.predict(vectorText)\n",
        "\n",
        "    if predictionText == orgPrediction:       #если кластеры одинаковые, то рассматриваем cer c порогом 50\n",
        "      cer = fastwer.score_sent(search, text, char_level=True)\n",
        "\n",
        "      if cer < 50:\n",
        "        print(row.original)"
      ],
      "metadata": {
        "id": "BXXcE8lkQFFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec & KMeans"
      ],
      "metadata": {
        "id": "GdAvnJtCTHvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_columns = [\"original\", \"transformed\"]"
      ],
      "metadata": {
        "id": "B5IU_iooTAjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfRaw = pd.read_csv(\"/content/gdrive/MyDrive/Сompanies/transformed_train.csv\")\n",
        "df = dfRaw.copy()"
      ],
      "metadata": {
        "id": "j9d3CIuQTQiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in text_columns:\n",
        "    df[col] = df[col].astype(str)"
      ],
      "metadata": {
        "id": "FpuSbeX0TV0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"tokens\"] = df[\"transformed\"].map(lambda x: word_tokenize(x))"
      ],
      "metadata": {
        "id": "AK3nmlpXTZ52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, idx = np.unique(df[\"tokens\"], return_index=True)       #удаляем повторяющиеся элементы\n",
        "df = df.iloc[idx, :]\n",
        "\n",
        "docs = df[\"original\"].values\n",
        "tokenized_docs = df[\"tokens\"].values"
      ],
      "metadata": {
        "id": "Dj4vvlhaTigN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=tokenized_docs, workers=1, seed=SEED)"
      ],
      "metadata": {
        "id": "9fYWJyutTuhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(listOfDocs, model):\n",
        "    features = []\n",
        "\n",
        "    for tokens in listOfDocs:\n",
        "        zeroVector = np.zeros(model.vector_size)\n",
        "        vectors = []\n",
        "        for token in tokens:\n",
        "            if token in model.wv:\n",
        "                try:\n",
        "                    vectors.append(model.wv[token])\n",
        "                except KeyError:\n",
        "                    continue\n",
        "        if vectors:\n",
        "            vectors = np.asarray(vectors)\n",
        "            avg_vec = vectors.mean(axis=0)\n",
        "            features.append(avg_vec)\n",
        "        else:\n",
        "            features.append(zeroVector)\n",
        "    return features\n",
        "    \n",
        "vectorized_docs = vectorize(tokenized_docs, model=model)      #берем среднее значение векторов"
      ],
      "metadata": {
        "id": "hfIykNSnTwzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mbkmeans_clusters(X, k, batch_size, print_silhouette_values):\n",
        "\n",
        "    km = MiniBatchKMeans(n_clusters=k, batch_size=batch_size).fit(X)\n",
        "\n",
        "    if print_silhouette_values:\n",
        "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
        "\n",
        "        silhouette_values = []\n",
        "        for i in range(k):\n",
        "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
        "            silhouette_values.append(\n",
        "                (\n",
        "                    i,\n",
        "                    cluster_silhouette_values.shape[0],\n",
        "                    cluster_silhouette_values.mean(),\n",
        "                    cluster_silhouette_values.min(),\n",
        "                    cluster_silhouette_values.max(),\n",
        "                )\n",
        "            )\n",
        "        silhouette_values = sorted(silhouette_values, key=lambda tup: tup[2], reverse=True)\n",
        "\n",
        "    return km, km.labels_"
      ],
      "metadata": {
        "id": "33xR1Hx8UDf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering, cluster_labels = mbkmeans_clusters(X=vectorized_docs, k=50, batch_size=500, print_silhouette_values=True)"
      ],
      "metadata": {
        "id": "JDOdJeaHUUP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clusters = pd.DataFrame({\"text\": docs, \"tokens\": [\" \".join(text) for text in tokenized_docs], \"cluster\": cluster_labels})"
      ],
      "metadata": {
        "id": "VgzbuQjyUswA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = vectorize([\"JX Nippon Oil & Gas Exploration (Brasil) Ltda\"], model=model)\n",
        "test_cluster = clustering.predict(X)"
      ],
      "metadata": {
        "id": "kxgSPw3gU-wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_representative_docs = np.argsort(np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1))\n",
        "\n",
        "for d in most_representative_docs[:10]:\n",
        "    print(docs[d])\n",
        "    print(\"-------------\")"
      ],
      "metadata": {
        "id": "UrNIqr6FVD8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}